export tensor_krylov, update_rhs!, KronMat, KronProd

using ExponentialUtilities: exponential!, expv

# Aliases
const KronProd{T}      = Vector{<:AbstractVector{T}} 
const KronMat{T}       = KroneckerMatrix{T}
const LowerTriangle{T} = LowerTriangular{T, <:AbstractMatrix{T}} 
const FMatrices{T}     = Vector{<:AbstractMatrix{T}} 


function exponentiate(A::AbstractMatrix{T}) where T <: AbstractFloat

    # Compute eigenvalues. Recall that the considered matrix is the Jacobi
    # matrix resulting from the Hermitian Lanczos algorithm. Therefore, the 
    # matrix is not necessarily Toepliz. However, we know that such a matrix
    # is of the form
    #   T = Uáµ€AU, where the eigenvalues of T are also eigenvalues of U. The 
    # question is then, which ones? The first k?


end

function get_subdiagonal_entries(A::KronMat, k::Int) 

    entries = [A[s][k + 1, k] for s in 1:length(A)]

    return entries

end

function matrix_exponential_vector!(
        y::ktensor,
        A::KronMat{T},
        b::KronProd{T},
        Î³::T,
        k::Int) where T<:AbstractFloat

    for s = 1:length(A)

        #y.fmat[s][:, k] = LinearAlgebra.BLAS.gemv('N' , exp(- Î³ .*  A[s]), b[s])

        tmp = Matrix(copy(A[s]))

        #y.fmat[s][:, k] = Î³ .* exponential!(tmp) * b[s]
        y.fmat[s][:, k] = expv(Î³, tmp, b[s])

    end

end

function innerprod_kronsum_tensor!(
        yX::FMatrices{T},
        yAx::FMatrices{T},
        Ax::FMatrices{T},
        x::ktensor,
        y::KronProd{T}) where T <: AbstractFloat

    # Computes <Ax, y>â‚‚, where A is a matrix (Kronecker sum) and y is a Kruskal tensor.
    mul!(yX, y, x)    
    mul!(yAx, y, Ax)  

    mask = trues(length(Ax))

    @assert length(Ax) == ndims(x)

    Ax_y = 0.0

    for s = 1:length(Ax)

        mask[s] = false

        yX_mask  = yX[mask]
        yAx_mask = yAx[.!mask]
        
        for i = 1:ncomponents(x)

            # Scale here with lambda
            Ax_y += x.lambda[i] * maskprod(yX_mask, i) * maskprod(yAx_mask, i)

        end

        mask[s] = true

    end

    return Ax_y

end

function solve_compressed_system(
        H::KronMat{T}, 
        b::Vector{<:AbstractVector{T}}, 
        Ï‰::Array{T},
        Î±::Array{T},
        t::Int,
        Î»::T,
    ) where T <: AbstractFloat

    reciprocal = inv(Î»)

    # Since we are considering a canonical decomposition the tensor rank of yâ‚œ
    # is equal to 

    k = dimensions(H)
    
    yâ‚œ = ktensor(reciprocal .* Ï‰, [ ones(k[s], t) for s in 1:length(H)] )

    for k = 1:t

        Î³ = -Î±[k] * reciprocal

        matrix_exponential_vector!(yâ‚œ, H, b, Î³, k)

    end

    return yâ‚œ
end

function compute_lower_outer!(L::AbstractMatrix{T}, Î³::Array{T}) where T <: AbstractFloat

    # Lower triangular matrix representing the outer product of a vector with itself

    t = size(L, 1)

    for j = 1:t, i = j:t

        L[i, j] = Î³[i] * Î³[j]

    end

end

function compute_coefficients(Î›::LowerTriangle{T}, Î´::Array{T}) where T <: AbstractFloat

    t = length(Î´)

    Î” = ones(t, t)

    # Lower triangle of outer product
    compute_lower_outer!(Î”, Î´) # âˆˆ â„áµ—áµ—

    Î“ = Î” .* Î›

    return Î“

end

function matrix_vector(
        A::KronMat{T},
        x::ktensor)::AbstractVector where T<:AbstractFloat

    # Compute the matrix vector products 
    #   
    #   zâ½Ë¢â¾áµ¢ = Aâ‚›â‹… xâ½Ë¢â¾áµ¢ for s = 1,â€¦,d, i = 1,â€¦,t
    #
    # This is equivalent as computing the product Zâ½Ë¢â¾ = Aâ‚›â‹…Xâ½Ë¢â¾, where Xâ½Ë¢â¾
    # are the factor matrices of the CP-tensor x.

    length(A) == ndims(x) || throw(DimensionMismatch("Kronecker matrix and vector (Kruskal tensor) have different number of components"))

    orders = [ size(A[s], 1) for s in 1:length(A) ]

    rank   = ncomponents(x)

    # Return vector of matrices as described above
    Z = [ zeros(orders[s], rank) for s in eachindex(A) ]

    @info A[1]

    for s = 1:length(A)

        LinearAlgebra.mul!(Z[s], A[s], x.fmat[s])

    end

    return Z

end

function skipindex(index::Int, range::UnitRange{Int})

    return Iterators.filter(r -> r != index, range)

end

function maskprod(A::FMatrices{T}, i::Int, j::Int) where T <: AbstractFloat

    # Compute product of entries (i,j) of the matrices contained in A.

    return prod(getindex.(A, i, j)) 

end


function maskprod(x::FMatrices{T}, i::Int) where T <: AbstractFloat

    return prod(getindex.(x, i)) 

end

function efficient_matrix_vector_norm(
        x::ktensor,
        Î›::AbstractMatrix{T},
        X_inner::FMatrices{T},
        Z::FMatrices{T}) where T <: AbstractFloat

    # Compute the squared 2-norm ||Ax||Â², where A âˆˆ â„á´ºÃ—á´º is a Kronecker sum and
    # x âˆˆ â„á´º is given as a Kruskal tensor of rank t.
    #
    # X_inner holds the inner products 
    #
    #   xáµ¢â½Ë¢â¾áµ€xâ±¼â½Ë¢â¾ for s = 1,â€¦,d, i,j = 1,â€¦,t
    #
    # And Z contains the matrices that represent the matrix vector products
    # 
    #   zâ½Ë¢â¾áµ¢ = Aâ‚›â‹… xâ½Ë¢â¾áµ¢ for s = 1,â€¦,d, i = 1,â€¦,t
    #
    # A is not passed explicitly, as the precomputed inner products are given.

    d      = ndims(x)
    rank   = ncomponents(x)

    # The following contain inner products of the form 
    #
    #   záµ¢â½Ë¢â¾áµ€zâ±¼â½Ë¢â¾ for s = 1,â€¦,d, i,j = 1,â€¦,t,
    # 
    # and 
    #
    #   záµ¢â½Ë¢â¾áµ€xâ±¼â½Ë¢â¾ for s = 1,â€¦,d, i,j = 1,â€¦,t,
    #
    # respcetively

    Z_inner = [ zeros(rank, rank) for _ in 1:d ]
    ZX      = [ zeros(rank, rank) for _ in 1:d ]

    compute_lower_triangles!(Z_inner, Z)

    #@info Z_inner

    for s in 1:d

        BLAS.gemm!('T', 'N', 1.0, Z[s], x.fmat[s], 1.0, ZX[s]) 

    end

    result = 0.0

    mask_s = trues(d)
    mask_r = trues(d)

    # We can separate the large sum 
    #
    #   Î£â‚›Î£áµ£Î£áµ¢Î£â±¼ xáµ¢â½Â¹â¾áµ€xâ±¼â½Â¹â¾ â‹¯ záµ¢â½Ë¢â¾áµ€xâ±¼â½Ë¢â¾ â‹¯ xáµ¢â½Ê³â¾áµ€zâ±¼â½Ê³â¾ â‹¯ xáµ¢â½áµˆâ¾áµ€xâ±¼â½áµˆâ¾
    #
    # into the cases 
    #
    #   (1) s  = r, i  = j,
    #   (2) s  = r, i != j,
    #   (3) s != r, i  = j,
    #   (4) s != r, i != j
    #
    # and simplify the calculation using the fact that some inner products 
    # appear twice (only access lower triangle of matrices) and that the norm
    # of the columns of the factor matrices are one.

    for s in 1:d

        for j = 1:rank # case (1)

            result += Î›[j, j] * Z_inner[s][j, j]

            mask_s[s] = false

            tmp = 0.0

            for i = skipindex(j, j:rank) # case (2)

                tmp += Î›[i, j] * maskprod(X_inner[mask_s], i, j) * maskprod(Z_inner[.!mask_s], i, j)

            end

            result += 2 * tmp

        end

        ZX_masked = ZX[.!mask_s]

        for r = skipindex(s, 1:d) # case (3)

            mask_r[r] = false

            mask_sr = mask_s .&& mask_r

            X_masked  = X_inner[mask_sr]
            XZ_masked =      ZX[.!mask_r]

            for i = 1:rank

                result += Î›[i, i] * ZX[s][i, i] * ZX[r][i, i]

                tmp = 0.0

                for j = skipindex(i, 1:rank) # case (4)

                    tmp += Î›[j, i] * maskprod(X_masked, i, j) *  maskprod(ZX_masked, i, j) * maskprod(XZ_masked, j, i)

                end

                result += 2 * tmp

            end

            mask_r[r] = true
        end

        mask_s[s] = true

    end

    return result

end


function compressed_residual(H::KronMat{T}, y::ktensor, b::KronProd{T}) where T<:AbstractFloat

    # This variant expands the matrices/tensors

    N = nentries(H)

    H_expanded = sparse(kroneckersum(H.ð–³...))
    y_expanded = reshape(full(y), N)
    b_expanded = kronecker(b...)

    @assert issparse(H_expanded)

    comp_res = (H_expanded * y_expanded) - b_expanded
    
    @info dot(comp_res, comp_res)
    return dot(comp_res, comp_res)

end

function squared_tensor_entries(Y_masked::FMatrices{T}, Î“::AbstractMatrix{T}) where T <: AbstractFloat

    # Compute Î£ |y_ð”|Â² with formula in paper, when y is given in CP format:
    #
    #   Î£ |y_ð”|Â² = ||Î£áµ¢ eâ‚–â‚›áµ€ yáµ¢â½Ë¢â¾ â¨‚ â±¼â‰  â‚› yáµ¢â½Ê²â¾||Â², 
    #
    # where Î´ represents the vector holding kâ‚›-th entry of each column of the 
    # s-th factor matrix of y.
    
    t = size(Y_masked, 1)

    value = 0.0

    for k = 1:t

        value += Î“[k, k] 

        for i = skipindex(k, k:t)

            value += 2 * Î“[i, k] * maskprod(Y_masked, i, k) # Symmetry

        end
    end

    return value 
end

function compute_lower_triangles!(LowerTriangles::FMatrices{T}, x::ktensor) where T<:AbstractFloat

    for s = 1:length(LowerTriangles)

        BLAS.syrk!('L', 'T', 1.0, x.fmat[s], 1.0, LowerTriangles[s])

    end

end

function compute_lower_triangles!(LowerTriangles::FMatrices{T}, x::FMatrices{T}) where T<:AbstractFloat

    for s = 1:length(LowerTriangles)

        BLAS.syrk!('L', 'T', 1.0, x[s], 1.0, LowerTriangles[s])

    end

end

function residual_norm(
        H::KronMat{T},
        y::ktensor,
        ð”Ž::Vector{Int},
        subdiagonal_entries::Vector{T},
        b::KronProd{T}) where T<:AbstractFloat
    
    # Compute squared norm of the residual according to Lemma 3.4 of paper.
    
    # Î£ |hË¢â‚–â‚Šâ‚â‚–|Â² * Î£ |y\_ð”|Â² + ||â„‹y - bÌƒ||Â²
    
    # Get entries at indices (kâ‚›+1, kâ‚›) for each dimension with pair of 
    # multiindices ð”Ž+1, ð”Ž

    d = length(H) # Number of dimensions

    t = ncomponents(y) # Tensor rank

    # Allocate memory for (lower triangular) matrices representing inner products
    Ly = [ zeros(t, t) for _ in 1:d ]

    compute_lower_triangles!(Ly, y)

    # Allocate memory for (lower triangular) matrix representing outer product
    # of coefficients.
    
    Î› = LowerTriangular(zeros(t, t))

    compute_lower_outer!(Î›, y.lambda)

    # Make matrices lower triangular
    Ly = map(LowerTriangular, Ly)

    res_norm = 0.0

    mask = trues(d)

    for s = 1:d

        Î“ = compute_coefficients(Î›, y.fmat[s][ð”Ž[s], :]) # Symmetric matrix 

        mask[s] = false

        yÂ² = squared_tensor_entries(Ly[.!mask], Î“)

        res_norm += abs( subdiagonal_entries[s] )^2 * yÂ²

        mask[s] = true

    end

    # Compute squared compressed residual norm
    #r_compressed = compressed_residual(Ly, Î›, H, y, b)
    r_compressed = compressed_residual(H, y, b)

    #@info r_compressed
    
    return sqrt(res_norm + r_compressed)

end

function update_rhs!(bÌƒ::KronProd{T}, V::KronProd{T}, b::KronProd{T}) where T<:AbstractFloat

    # bÌƒ = Váµ€b = â¨‚ Vâ‚›áµ€ â‹… â¨‚ bâ‚› = â¨‚ Vâ‚›áµ€bâ‚›
    
    for s = 1:length(bÌƒ)

        # Update one entry of each component of bÌƒ by performing a single inner product 
        bÌƒ[s][end] = dot( V[s] , b[s] )
     
    end

end

function basis_tensor_mul!(x::ktensor, V::KronMat{T}, y::ktensor) where T<:AbstractFloat

    x.lambda = copy(y.lambda)

    for s in eachindex(V)

        LinearAlgebra.mul!(x.fmat[s], V[s], y.fmat[s])

    end

end

function initialize(
        A::KronMat{T},
        b::KronProd{T},
        orthonormalization::Type{<:TensorDecomposition}) where T <: AbstractFloat


    # Initialize the d Arnoldi decompositions of Aâ‚›
    tensor_decomposition = orthonormalization{T}(A)

    # Initialize multiindex ð”Ž
    ð”Ž = Vector{Int}(undef, d)

    # Allocate memory for right-hand side bÌƒ
    bÌƒ = [ zeros( size(b[s]) )  for s in eachindex(b) ]

    orthonormal_basis!(tensor_decomposition, b, 1, orthonormalization)

end


function tensor_krylov(
        A::KronMat{T},
        b::KronProd{T},
        tol::T,
        nmax::Int,
        orthonormalization::Type{<:TensorDecomposition}) where T <: AbstractFloat

	# Initilialize implicit tensorized Krylov subspace basis and upper Hessenberg 
    d = length(A)

    # Initialize the d Arnoldi decompositions of Aâ‚›
    tensor_decomposition = orthonormalization{T}(A)

    # Initialize multiindex ð”Ž
    ð”Ž = Vector{Int}(undef, d)

    # Allocate memory for right-hand side bÌƒ
    bÌƒ = [ zeros( size(b[s]) )  for s in eachindex(b) ]

    #@info "bÌƒ after update" bÌƒ

    # Allocate memory for approximate solution
    #x = ktensor( ones(1), [ zeros(size(A[s], 1), 1) for s in 1:d ] )
    x = nothing

    coefficients_df = compute_dataframe()

    orthonormal_basis!(tensor_decomposition, b, 1, orthonormalization)

    H_minors = principal_minors(tensor_decomposition.H, 1)
    V_minors = principal_minors(tensor_decomposition.V, 1)
    b_minors = principal_minors(bÌƒ, 1)

    y = inv( sum(H_minors[1, 1]) ) * prod( [ b_minors[s][1] for s in 1:d ] )

    x = prod(V_minors[1, 1]) * y

    for k = 2:nmax

        # Compute orthonormal basis and Hessenberg factor of each Krylov subspace ð“šâ‚–(Aâ‚›, bâ‚›) 
        orthonormal_basis!(tensor_decomposition, b, k, orthonormalization)

        H_minors = principal_minors(tensor_decomposition.H, k)
        V_minors = principal_minors(tensor_decomposition.V, k)
        b_minors = principal_minors(bÌƒ, k)

        columns = kth_columns(tensor_decomposition.V, k)

        # Update compressed right-hand side bÌƒ = Váµ€b
        update_rhs!(b_minors, columns, b)

        b_norm = kronprodnorm(b_minors)

        Î»_min, Î»_max = projected_kronecker_eigenvalues(H_minors)

        Îº = Î»_max / Î»_min

        Ï‰, Î±, rank = optimal_coefficients(coefficients_df, tol, Îº, Î»_min, b_norm)

        # Approximate solution of compressed system
        y = solve_compressed_system(H_minors, b_minors, Ï‰, Î±, rank, Î»_min)

        ð”Ž .= k 

        subdiagonal_entries = [ tensor_decomposition.H[s][k + 1, k] for s in 1:d ]

        @info subdiagonal_entries

        # Compute residual norm
        r_norm = residual_norm(H_minors, y, ð”Ž, subdiagonal_entries, b_minors)

        rel_res_norm = (r_norm / kronprodnorm(b_minors))

        @info "Iteration: " k "relative residual norm:" rel_res_norm


        if rel_res_norm < tol

            x = ktensor( ones(rank), [ zeros(size(A[s], 1), rank) for s in 1:d ])

            x_minors = principal_minors(x, k)

            basis_tensor_mul!(x_minors, V_minors, y)

            println("Convergence")

            return x

        end

    end

    println("No convergence")

end
