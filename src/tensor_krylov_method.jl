function mul!(
        result::Vector{Array{T}},
        KP::Vector{Array{T}},
        x::ktensor) where T <: AbstractFloat

    náµ¢ = ndims(x)

   for s = 1:náµ¢

       result[s] = transpose(KP[s]) * (x.lambda[s] * x.fmat[s])

   end

end

function mul!(
        result::Vector{Array{T}},
        KP::Vector{Array{T}},
        X::Vector{Matrix{T}}) where T <: AbstractFloat

    náµ¢ = length(result)

   for s = 1:náµ¢

       result[s] = transpose(KP[s]) * X.fmat[s]

   end

end

function matrix_exponential_vector!(
        factors::AbstractVector,
        A::KroneckerMatrix{T},
        b::Vector{Array{T}}, Î³::T) where T<:AbstractFloat

    for s = 1:length(A)

        factors[s] = LinearAlgebra.BLAS.gemv('N' , exp(- Î³ * A[s]), b[s] )

    end

end

#function solve_compressed_system(
#        H::KroneckerMatrix{T}, 
#        b::Vector{Array{T}}, 
#        Ï‰::Array{T},
#        Î±::Array{T},
#        t::Int) where T <: AbstractFloat
#
#    Î» = smallest_eigenvalue(H) # This might be different depending on the system
#
#    reciprocal = inv(Î»)
#
#    # Since we are considering a canonical decomposition the tensor rank of yâ‚œ
#    # is equal to 
#    
#    yâ‚œ = ktensor(reciprocal .* Ï‰, [ ones(t,t) for _ in 1:length(H)] )
#    
#    for j = 1:t
#
#        Î³ = -Î±[j] * reciprocal
#
#        matrix_exponential_vector!(yâ‚œ.fmat, H, b, Î³)
#
#    end
#
#    return yâ‚œ
#end

function hessenberg_subdiagonals(H::AbstractMatrix, ð”Ž::Vector{Int})

    # Extract subdiagonal entries (kâ‚›â‚Šâ‚, kâ‚›) of matrix Hâ½Ë¢â¾ of â„‹     

    d = length(ð”Ž)

    entries = Array{Float64}(undef, d)

    for s = 1:d

        entries[s] = H[ð”Ž[s] + 1, ð”Ž[s]]

    end

    return entries

end

function compute_lower_triangle!(
        LowerTriangle::LowerTriangular{T, Matrix{T}},
        Î³::Array{T}) where T <: AbstractFloat

    t = size(LowerTriangle, 1)

    for j = 1:t, i = j:t

        LowerTriangle[i, j] = Î³[i] * Î³[j]

    end

end

function compute_coefficients(
        Î›::LowerTriangular{T, Matrix{T}},
        Î´::Vector{T}) where T <: AbstractFloat

    
    t = length(Î´)

    Î” = ones(t, t)

    # Lower triangle of outer product
    compute_lower_triangle!(Î”, Î´) # âˆˆ â„áµ—áµ—

    Î“ = Î” .* Î›

    return Î“

end

function matrix_vector(
        A::KroneckerMatrix{T},
        x::ktensor)::AbstractVector where T<:AbstractFloat

    # Compute the matrix vector products 
    #   
    #   zâ½Ë¢â¾áµ¢ = Aâ‚›â‹… xâ½Ë¢â¾áµ¢ for s = 1,â€¦,d, i = 1,â€¦,t
    #
    # This is equivalent as computing the product Zâ½Ë¢â¾ = Aâ‚›â‹…Xâ½Ë¢â¾, where Xâ½Ë¢â¾
    # are the factor matrices of the CP-tensor x.

    orders = dimensions(A)
    rank   = ncomponents(x)

    # Return vector of matrices as described above
    Z = [ zeros(orders[s], rank) for s in eachindex(A) ]

    for s = 1:length(A)

        LinearAlgebra.mul!(Z[s], A[s], x.fmat[s])

    end

    return Z

end

function skipindex(index::Int, range::UnitRange{Int})

    return Iterators.filter(r -> r != index, range)

end

function mask_prod(A::Vector{Matrix{T}}, i::Int, j::Int) where T <: AbstractFloat

    return prod(getindex.(A, i, j)) 

end

function mask_prod(x::Vector{Array{T}}, i::Int) where T <: AbstractFloat

    return prod(getindex.(x, i)) 

end

function efficient_matrix_vector_norm(
        x::ktensor,
        Î›::Matrix{T},
        X_inner::Vector{Matrix{T}},
        Z::Vector{Matrix{T}}) where T <: AbstractFloat

    # Compute the squared 2-norm ||Ax||Â², where A âˆˆ â„á´ºÃ—á´º is a Kronecker sum and
    # x âˆˆ â„á´º is given as a Kruskal tensor of rank t.
    #
    # X_inner holds the inner products 
    #
    #   xáµ¢â½Ë¢â¾áµ€xâ±¼â½Ë¢â¾ for s = 1,â€¦,d, i,j = 1,â€¦,t
    #
    # And Z contains the matrices that represent the matrix vector products
    # 
    #   zâ½Ë¢â¾áµ¢ = Aâ‚›â‹… xâ½Ë¢â¾áµ¢ for s = 1,â€¦,d, i = 1,â€¦,t
    #
    # A is not passed explicitly, as the precomputed inner products are given.

    d      = ndims(x)
    rank   = ncomponents(x)

    # The following contain inner products of the form 
    #
    #   záµ¢â½Ë¢â¾áµ€zâ±¼â½Ë¢â¾ for s = 1,â€¦,d, i,j = 1,â€¦,t,
    # 
    # and 
    #
    #   záµ¢â½Ë¢â¾áµ€xâ±¼â½Ë¢â¾ for s = 1,â€¦,d, i,j = 1,â€¦,t,
    #
    # respcetively

    Z_inner = [ zeros(rank, rank) for s in 1:d ]
    ZX      = [ zeros(rank, rank) for s in 1:d ]

    for s in 1:d

        BLAS.syrk!('L', 'T', 1.0, Z[s], 1.0,  Z_inner[s])      # Compute only lower triangle
        BLAS.gemm!('T', 'N', 1.0, Z[s], x.fmat[s], 1.0, ZX[s]) 

    end

    result = 0.0

    mask_s = trues(d)
    mask_r = trues(d)

    # We can separate the large sum 
    #
    #   Î£â‚›Î£áµ£Î£áµ¢Î£â±¼ xáµ¢â½Â¹â¾áµ€xâ±¼â½Â¹â¾ â‹¯ záµ¢â½Ë¢â¾áµ€xâ±¼â½Ë¢â¾ â‹¯ xáµ¢â½Ê³â¾áµ€zâ±¼â½Ê³â¾ â‹¯ xáµ¢â½áµˆâ¾áµ€xâ±¼â½áµˆâ¾
    #
    # into the cases 
    #
    #   (1) s  = r, i  = j,
    #   (2) s != r, i  = j,
    #   (3) s  = r, i != j,
    #   (4) s != r, i != j
    #
    # and simplify the calculation using the fact that some inner products 
    # appear twice (only access lower triangle of matrices) and that the norm
    # of the columns of the factor matrices are one.

    for s in 1:d

        for j = 1:rank

            result += Î›[j, j] * Z_inner[s][j, j]

            mask_s[s] = false

            for i = skipindex(j, j:rank)

                result += 2 * Î›[i, j] * mask_prod(X_inner[mask_s], i, j) * Z_inner[.!mask_s][1][i, j]

            end

        end

        for r = skipindex(s, 1:d)

            mask_r[r] = false

            for i = 1:rank

                result += Î›[i, i] * ZX[s][i, i] * ZX[r][i, i]

                for j = skipindex(i, 1:rank)

                    result += Î›[j, i] * mask_prod(ZX[mask_s .&& mask_r], i, j) * mask_prod(ZX[.!(mask_s .&& mask_r)], j, i)

                end

            end

            mask_r[r] = true
        end

        mask_s[s] = true

    end

    return result

end


function compressed_residual(
        Ly::Vector{LowerTriangular{T, Matrix{T}}},
        Î›::LowerTriangular{T, Matrix{T}},
        H::KroneckerMatrix{T},
        y::ktensor,
        b::Vector{Array{T}}) where T <:AbstractFloat

    # TODO: 
    #
    # We know that 
    #
    # ||Hy - b||Â² = ||Hy||Â² -2â‹…báµ€(Hy) + ||b||Â² 
    
    d = length(H)
    t = ncomponents(y)

    # For this we evaluate all Bâ½Ë¢â¾[:, i] = Hâ‚›yâ½Ë¢â¾áµ¢ âˆˆ â„â¿â‚› for i = 1,â€¦,t
    B = matrix_vector(H, y)

    # First we compute ||Hy||Â²
    Hy_norm = efficient_matrix_vector_norm(y, Î›, Ly, B)

    # Now we compute <Hy, b>â‚‚
    Hy_b = 0.0

    bY = [ zeros(t) for _ in 1:d ]
    bZ = [ zeros(t) for _ in 1:d ]

    mul!(bY, b, y)
    mul!(bZ, b, B)

    mask = trues(d)

    for s = 1:d, i = 1:t

        mask[s] = false

        Hy_b += mask_prod(bY[mask], i) * mask_prod(bZ[.!mask], i)

    end

    # Finally we compute the 2-norm of b
    b_norm = prod( norm(b[s]) for s in 1:d )

    return Hy_norm - 2 * Hy_b + b_norm
    
end

function squared_tensor_entries(
        Y_masked::Vector{LowerTriangular{T, Matrix{T}}},
        Î“::LowerTriangular{T, Matrix{T}}) where T <: AbstractFloat

    # Compute Î£ |y_ð”|Â² with formula in paper, when y is given in CP format:
    #
    #   Î£ |y_ð”|Â² = ||Î£áµ¢ eâ‚–â‚›áµ€ yáµ¢â½Ë¢â¾ â¨‚ â±¼â‰  â‚› yáµ¢â½Ê²â¾||Â², 
    #
    # where Î´ represents the vector holding kâ‚›-th entry of each column of the 
    # s-th factor matrix of y.
    
    t = size(Y_masked, 1)

    value = 0.0

    for k = 1:t

        value += Î“[k, k] 

        for i = skipindex(k, k:t)

            value += Î“[i, k] * mask_prod(Y_masked, i, k)

        end
    end

    return 2 * value # Symmetry of inner products
end

    
function residual_norm(H::KroneckerMatrix, y::ktensor, ð”Ž::Vector{Int}, b)
    
    # Compute norm of the residual according to Lemma 3.4 of paper.
    
    # Î£ |hË¢â‚–â‚Šâ‚â‚–|Â² * Î£ |y\_ð”|Â² + ||â„‹y - bÌƒ||Â²
    
    # Get entries at indices (kâ‚›+1, kâ‚›) for each dimension with pair of 
    # multiindices ð”Ž+1, ð”Ž

    d = size(H) # Number of dimensions

    t = ncomponents(y) # Tensor rank

    # Subdiagonal entries of upper Hesseberg matrices
    hÂ² = map(abs, hessenberg_subdiagonals(H, ð”Ž)).^2

    # Allocate memory for (lower triangular) matrices representing inner products
    Ly = [ zeros(t, t) for _ in 1:d ]

    for s = 1:d

        BLAS.syrk!('L', 'T', 1.0, y.fmat[s], 1.0, Ly[s]) # Only need lower triangle

    end

    # Allocate memory for (lower triangular) matrix representing outer product
    # of coefficients.
    
    Î› = LowerTriangular(zeros(t, t))

    Î› = compute_lower_triangle!(Î›, y.lambda)

    res_norm = 0.0

    mask = trues(d)

    for s = 1:d

        Î“ = compute_coefficients(Î›, y.fmat[s][ð”Ž[s], :])

        mask[s] = false

        yÂ² = squared_tensor_entries(Ly[mask], Î“)

        res_norm += hÂ²[s] * yÂ²

        mask[s] = true

    end

    # Compute squared compressed residual norm
    râ‚• = compressed_residual(Ly, Symmetric(Î›, :L), H, y, b)

    return res_norm + râ‚•

end

function tensor_krylov(A::KroneckerMatrix, b::AbstractVector, tol) 

	# Initilialize implicit tensorized Krylov subspace basis and upper Hessenberg 
	# matrix of compressed linear system
	decompositions = [Arnoldi(A[s], size(A[s], 1)) for s = 1:length(A)]

	# Compute basis and Hessenberg factor of each Krylov subspace ð“šâ‚–(Aâ‚›, bâ‚›) 
	for s = 1:length(A)
		
		arnoldi_modified!(A[s], b[s], 100, decompositions[s])
		
	end


    #H = KroneckerMatrix(decompositions)

    
    #y = solve_compressed_system()

	return decompositions
end
