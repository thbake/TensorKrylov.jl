function mul!(
        result::Vector{Array{T}},
        KP::Vector{Array{T}},
        x::ktensor) where T <: AbstractFloat

    náµ¢ = ndims(x)

   for s = 1:náµ¢

       result[s] = transpose(KP[s]) * (x.lambda[s] * x.fmat[s])

   end

end

function mul!(
        result::Vector{Array{T}},
        KP::Vector{Array{T}},
        X::Vector{Matrix{T}}) where T <: AbstractFloat

    náµ¢ = length(result)

   for s = 1:náµ¢

       result[s] = transpose(KP[s]) * X.fmat[s]

   end

end

function matrix_exponential_vector!(
        factors::AbstractVector,
        A::KroneckerMatrix{T},
        b::Vector{Array{T}}, Î³::T) where T<:AbstractFloat

    for s = 1:length(A)

        factors[s] = LinearAlgebra.BLAS.gemv('N' , exp(- Î³ * A[s]), b[s] )

    end

end

#function solve_compressed_system(
#        H::KroneckerMatrix{T}, 
#        b::Vector{Array{T}}, 
#        Ï‰::Array{T},
#        Î±::Array{T},
#        t::Int) where T <: AbstractFloat
#
#    Î» = smallest_eigenvalue(H) # This might be different depending on the system
#
#    reciprocal = inv(Î»)
#
#    # Since we are considering a canonical decomposition the tensor rank of yâ‚œ
#    # is equal to 
#    
#    yâ‚œ = ktensor(reciprocal .* Ï‰, [ ones(t,t) for _ in 1:length(H)] )
#    
#    for j = 1:t
#
#        Î³ = -Î±[j] * reciprocal
#
#        matrix_exponential_vector!(yâ‚œ.fmat, H, b, Î³)
#
#    end
#
#    return yâ‚œ
#end

function hessenberg_subdiagonals(H::AbstractMatrix, ğ”::Vector{Int})

    # Extract subdiagonal entries (kâ‚›â‚Šâ‚, kâ‚›) of matrix Hâ½Ë¢â¾ of â„‹     

    d = length(ğ”)

    entries = Array{Float64}(undef, d)

    for s = 1:d

        entries[s] = H[ğ”[s] + 1, ğ”[s]]

    end

    return entries

end

function compute_lower_triangle!(
        LowerTriangle::LowerTriangular{T, Matrix{T}},
        A::Matrix{T},
        B::Matrix{T},
        Î³::Array{T},
        k::Int) where T <: AbstractFloat

    t = length(Î³)

    for j = 1:t-k, i = j+k:t

        LowerTriangle[i, j] = (Î³[i]*Î³[j])dot(@view(A[:, j]), @view(B[:, i]))
        
    end

end

function compute_lower_triangle!(
        LowerTriangle::LowerTriangular{T, Matrix{T}},
        A::Matrix{T},
        k::Int) where T <: AbstractFloat

    t = size(A, 2)

    for j = 1:t-k, i = j+k:t

        LowerTriangle[i, j] = dot(@view(A[:, j]), @view(A[:, i]))
        
    end

end

function compute_lower_triangle!(
        LowerTriangle::LowerTriangular{T, Matrix{T}},
        Î³::Array{T}) where T <: AbstractFloat

    t = size(LowerTriangle, 1)

    for j = 1:t, i = j:t

        LowerTriangle[i, j] = Î³[i] * Î³[j]

    end

end

function compute_coefficients(
        LowerTriangle::LowerTriangular{T, Matrix{T}},
        Î´::Vector{T}) where T <: AbstractFloat

    # Compute Î£ |y_ğ”|Â² with formula in paper, when y is given in CP format:
    #
    # Î£ |y_ğ”|Â² = ||Î£áµ¢ eâ‚–â‚›áµ€ yáµ¢â½Ë¢â¾ â¨‚ â±¼â‰  â‚› yáµ¢â½Ê²â¾||Â².
    
    # Get the kâ‚›-th entry of each column of the s-th factor matrix of y.
    t = length(Î´)

    Î” = ones(t, t)

    compute_lower_triangle!(Î”, Î´) # âˆˆ â„áµ—áµ—

    Î“ = Î” .* LowerTriangle

    return Î“

end

function innerproducts!(
        LowerTriangles::Vector{LowerTriangular{T, Matrix{T}}},
        factormatrix,
        k::Int) where T <: AbstractFloat

    for s in eachindex(LowerTriangles)

        compute_lower_triangle!(LowerTriangles[s], factormatrix[s], k)
    end
    
end

function matrix_vector(
        A::KroneckerMatrix{T},
        x::ktensor)::AbstractVector where T<:AbstractFloat

    # Compute the matrix vector products 
    #   
    #   zâ½Ë¢â¾áµ¢ = Aâ‚›â‹… xâ½Ë¢â¾áµ¢ for s = 1,â€¦,d, i = 1,â€¦,t
    #
    # This is equivalent as computing the product Zâ½Ë¢â¾ = Aâ‚›â‹…Xâ½Ë¢â¾, where Xâ½Ë¢â¾
    # are the factor matrices of the CP-tensor x.

    orders = dimensions(A)
    rank   = ncomponents(x)

    # Return vector of matrices as described above
    Z = [ zeros(orders[s], rank) for s in eachindex(A) ]

    for s = 1:length(A)

        LinearAlgebra.mul!(Z[s], A[s], x.fmat[s])

    end

    for s = 1:length(A)

        # Scale columns of each matrix with entries of lambda
        Z[s] = x.lambda' .* Z[s] 

    end

    return Z

end

function skipindex(index::Int, range::UnitRange{Int})

    return Iterators.filter(r -> r != index, range)

end

function ktensor_innerprods!(
        Lx::Vector{LowerTriangular{T, Matrix{T}}}, 
        x::ktensor) where T <: AbstractFloat

    t = ncomponents(x)

    # Allocate memory for and initialize matrix representing scaling of factor 
    # matrices in the CP decomposition
    Î› = LowerTriangular( ones(t, t) )

    compute_lower_triangle!(Î›, x.lambda)

    # Compute (lower triangular) matrices representing inner products
    innerproducts!(Lx, x.fmat, 1)

    # Scale with norms of CP decomposition
    map(X -> Î› .* X, Lx)


end

function squared_matrix_vector(
        Lx::Vector{LowerTriangular{T, Matrix{T}}},
        Z::Vector{Matrix{T}},
        A::KroneckerMatrix{T}, 
        x::ktensor)::T where T <: AbstractFloat

    # Computes the squared norm ||Ax||Â², where Ax = z
    # Z is already scaled by Î»áµ¢ of the tensor x

    d = length(A)
    t = ncomponents(x)

    Lz = [ LowerTriangular(ones(t, t)) for _ in 1:d ]

    # Compute (lower triangular) matrix represeting inner products zâ½Ë¢â¾áµ¢áµ€zâ½Ë¢â¾â±¼
    innerproducts!(Lz, Z, 0)

    #alpha_vector = (x.lambda).^2
    Î± = (x.lambda).^2

    for i = 1:t, s = 1:d

        #Î± = alpha_vector .* (d - 1)

        Lz[s][i, i] = Lz[s][i, i] * Î±[i]

    end

    # Case 1: s = r, i = j:
    # Only sum over the squared 2-norms of zâ½Ë¢â¾áµ¢ for i = 1,â€¦,t
    squared_norm = sum( tr(Lz[s]) for s in eachindex(Lz) )

    # Case 2: s = r, i != j:
    # Sum over dot d-1 dot products of the form xâ½Ê³â¾áµ¢áµ€ xâ½Ê³â¾â±¼ times zâ½Ë¢â¾áµ¢áµ€ zâ½Ë¢â¾â±¼ 
    for s = 1:d, r = skipindex(s, 1:d)

        for j = 1:t-1, i = j+1:t

            squared_norm += Lx[r][i, j] * Lz[s][i, j]

        end

    end

    # Here we count twice because of symmetry of the inner products
    squared_norm *= 2

    # Case 3: s != r, i = j:
    # Only compute two inner products zâ½Ê³â¾áµ¢áµ€ xâ½Ê³â¾áµ¢ times xâ½Ë¢â¾áµ¢áµ€ zâ½Ë¢â¾áµ¢ and sum
    # over them


    XZ = [ zeros(t,t) for _ in 1:d ]

    (mul!(XZ[s], transpose(@view(Z[s])), x.fmat[s]) for s = 1:d)

    for s = 1:d, r = skipindex(s, 1:d) 

        for i = 1:t

            squared_norm += XZ[r][i, i] * XZ[s][i, i]

        end

    end

    # Case 4: s != r, i != j:
    # Compute rest of inner products 

    tmp = 0.0

    for s = 1:d, r = skipindex(s, 1:d)

        for j = 1:t, i = skipindex(j , 1:t)

            tmp += XZ[r][i, j] * Lx[s][i, j] 

        end

    end

    squared_norm += 2 * tmp

    return squared_norm

end

# Last try of squared norm
function lastnorm(A::KroneckerMatrix{T}, x::ktensor) where T<:AbstractFloat

    orders = dimensions(A)
    d      = ndims(x)
    rank   = ncomponents(x)

    # Return vector of matrices as described above
    Z = [ zeros(orders[s], rank) for s in 1:d ]

    for s = 1:length(A)

        LinearAlgebra.mul!(Z[s], A[s], x.fmat[s])

    end

    X_inner = [ ones(rank, rank) for s in 1:d ]
    Z_inner = [ ones(rank, rank) for s in 1:d ]
    XZ      = [ ones(rank, rank) for s in 1:d ]
    ZX      = [ ones(rank, rank) for s in 1:d ]
    

    for s = 1:length(A)

        LinearAlgebra.mul!(X_inner[s], transpose(x.fmat[s]), x.fmat[s])
        LinearAlgebra.mul!(Z_inner[s], transpose(Z[s]), Z[s])
        LinearAlgebra.mul!(XZ[s], transpose(x.fmat[s]), Z[s])
        ZX[s] = transpose(copy(XZ[s]))

    end

    my_norm = 0.0

    mask_s = trues(d)
    mask_r = trues(d)

    for s in 1:d

        for i = 1:rank

            my_norm += x.lambda[i]^2 * Z_inner[s][i, i]

            mask_s[s] = false

            for j = skipindex(i, 1:rank)

                my_norm += x.lambda[i] * x.lambda[j] * prod(getindex.(X_inner[mask_s], i, j)) * Z_inner[.!mask_s][1][i, j]

            end

        end

        for r = skipindex(s, 1:d)

            mask_r[r] = false

            for i = 1:rank

                my_norm += x.lambda[i]^2 * ZX[s][i, i] * XZ[s][i, i]

                for j = skipindex(i, 1:rank)

                    my_norm += x.lambda[i] * x.lambda[j] * prod(getindex.(ZX[mask_r], i, j)) * XZ[.!mask_r][1][i, j]

                end

            end

            mask_r[r] = true
        end

        mask_s[s] = true
    end

    return my_norm

end

function compressed_residual(
        Ly::Vector{LowerTriangular{T, Matrix{T}}},
        H::KroneckerMatrix{T},
        y::ktensor,
        b::Vector{Array{T}}) where T <:AbstractFloat

    # TODO: 
    #
    # We know that 
    #
    # ||Hy - b||Â² = ||Hy||Â² -2â‹…báµ€(Hy) + ||b||Â² 
    
    d = length(H)
    t = ncomponents(y)

    # For this we evaluate all Bâ½Ë¢â¾[:, i] = Hâ‚›yâ½Ë¢â¾áµ¢ âˆˆ â„â¿â‚›Ë£áµ—
    B = matrix_vector(H, y)

    # First we compute ||Hy||Â²
    Hy_norm = squared_matrix_vector(Ly, B, H, y)

    # Now we compute <Hy, b>â‚‚

    Hy_b = 0.0

    bY = [ zeros(t) for _ in 1:d ]
    bZ = [ zeros(t) for _ in 1:d ]


    mul!(bY, b, y)
    mul!(bZ, b, B)
    


    for s = 1:d, r = skipindex(s, 1:d), i = 1:t

        Hy_b += bY[r][i] * bY[s][i]

    end

    # Finally we compute the 2-norm of b
    b_norm = norm(b)

    return Hy_norm - 2 * Hy_b + b_norm
    
    
end

    
function residual_norm(H::KroneckerMatrix, y::ktensor, ğ”::Vector{Int}, b)
    
    # Compute norm of the residual according to Lemma 3.4 of paper.
    
    # Î£ |hË¢â‚–â‚Šâ‚â‚–|Â² * Î£ |y\_ğ”|Â² + ||â„‹y - bÌƒ||Â²
    
    # Get entries at indices (kâ‚›+1, kâ‚›) for each dimension with pair of 
    # multiindices ğ”+1, ğ”

    # Number of dimensions
    d = size(H)

    # Tensor rank
    t = ncomponents(y)

    # Extract subdiagonal entries of upper Hesseberg matrices
    hÂ² = map(abs, hessenberg_subdiagonals(H, ğ”)).^2

    # Allocate memory for (lower triangular) matrices representing inner products
    Ly = [ LowerTriangular(ones(t, t)) for _ in 1:d ]

    ktensor_innerprods!(Ly, y)

    res_norm = 0.0

    for s = 1:d

        yÂ² = 0.0

        C = compute_coefficients(Ly[s], y.fmat[s][ğ”[s], :])

        for k = 1:t, i = k:t

            product = 1.0

            for j = skipindex(s, 1:d)

                product *= Ly[j][i, k]

            end

            yÂ² += C[i, k] * product

        end

        # Here I'm counting the diagonal twice... Need to account for that.
        yÂ² *= 2.0

        res_norm += hÂ²[s] * yÂ²

    end


    # Compute squared compressed residual norm
    râ‚• = compressed_residual(Ly, H, y, b)

    return res_norm + râ‚•

end

function multiple_hadamard(S::Vector{Matrix{T}}) where T<:AbstractFloat

    U = ones(size(S[1]))

    for s in eachindex(S)

        U .*= S[s]

    end

    return U

end

function squared_norm_vectorized(x::ktensor, A::KroneckerMatrix{T}) where T<:AbstractFloat

    d = ndims(x)
    t = ncomponents(x)
    orders = dimensions(A)

    Z = [ zeros(orders[s], t) for s in eachindex(A) ]

    for s = 1:length(A)

        LinearAlgebra.mul!(Z[s], A[s], x.fmat[s])

    end

    ZZ = [ Z[s]'Z[s] for s = 1:d ]

    Î› = x.lambda * x.lambda'

    X = [ x.fmat[s]'x.fmat[s] for s = 1:d ]

    XZ = [ x.fmat[s]'Z[s] for s = 1:d ]

    ZX = [ Z[s]'x.fmat[s] for s = 1:d ]

    mask = falses(d)
    mask_s = falses(d)
    mask_r = falses(d)

    A = ones(t,t)
    B = ones(t,t)
    C = ones(t,t)


    for s = 1:d

        mask_s[s] = true

        for r = 1:d

            mask_r[r] = true

            if r == s

                A = ZZ[mask_s][1]

            else
                # Yields Zâ½Ë¢â¾áµ€Xâ½Ë¢â¾, and Xâ½Ê³â¾áµ€Zâ½Ê³â¾
                A = ZX[mask_s][1] .* XZ[mask_r][1]

            end

            B = multiple_hadamard(X[.!(mask_s .&& mask_r)]) 

            C += Î› .* A .* B

            mask_r[r] = false

        end

        mask_s[s] = false

    end

    return sum(C)

end

function tensor_krylov(A::KroneckerMatrix, b::AbstractVector, tol) 

	# Initilialize implicit tensorized Krylov subspace basis and upper Hessenberg 
	# matrix of compressed linear system
	decompositions = [Arnoldi(A[s], size(A[s], 1)) for s = 1:length(A)]

	# Compute basis and Hessenberg factor of each Krylov subspace ğ“šâ‚–(Aâ‚›, bâ‚›) 
	for s = 1:length(A)
		
		arnoldi_modified!(A[s], b[s], 100, decompositions[s])
		
	end


    #H = KroneckerMatrix(decompositions)

    
    #y = solve_compressed_system()

	return decompositions
end
